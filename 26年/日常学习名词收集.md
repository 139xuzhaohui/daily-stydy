
---------
Serverless
- 
-  还记得吗？
---------
vLLM
- 
- 是为了干啥的，我记得加速模型的推理能力：
- vLLM 的设计目标与特点
- vLLM 是一个高性能的大语言模型服务框架。在大语言模型日益普及的今天，如何高效地提供推理服务成为一个重要挑战。传统的服务框架在处理并发请求时往往会遇到 **性能瓶颈、内存管理效率低下**等问题。vLLM 正是为解决这些关键挑战而生。
    - 在**性能优化**方面，vLLM 最大的创新在于引入了突破性的 **PagedAttention** 机制。这项技术巧妙地将传统的**连续注意力计算改造为基于页**的形式，使得 GPU 内存的利用效率得到极大提升。通过精心设计的**连续批处理机制**，vLLM 能够充分利用 GPU 算力，显著提升推理速度。
    - 说到内存效率，vLLM 采用了独特的**动态内存管理方案**。通过对 **KV Cache 的动态管理和页式存储**，它能够 **有效减少内存碎片**，提高内存利用率。这就像是在有限的空间里实现了"完美俄罗斯方块"，让每一块内存都物尽其用。
    - 在**并发处理**能力上，vLLM 表现同样出色。它采用**灵活的调度策略**， *【是啥？？？】*， 能够同时处理大量并发请求，并通过动态负载均衡确保资源的最优分配。这种设计让 vLLM 能够轻松应对高并发场景，就像一个经验丰富的交通指挥官，让所有请求都能高效有序地通行。
    - 作为一个服务框架，vLLM 的架构设计非常灵活。它不仅支持单机部署，还能轻松**扩展到分布式环境**。**框架兼容主流的 LLM 模型**，并提供简洁的 API 接口，大大降低了开发者的使用门槛。 *【模型对于我们来说到底是什么？？？】*
    - 在实际应用中，vLLM 展现出了令人印象深刻的性能表现。相比传统方案，它能够将**推理服务的吞吐量提升 2-4 倍，同时显著降低请求延迟**。这种性能提升不仅体现在数据上，更为用户带来了明显的体验改善。
    - 对开发者而言，vLLM 提供了完善的工具支持。从简单的集成方式，到**丰富的监控指标和调优选项**，都体现了框架在易用性方面的深思熟虑。这使得开发者能够快速上手，并根据实际需求进行灵活调整。
---------
推理引擎
- 
- 随着大语言模型（LLM）在各类业务场景中的广泛应用，选择合适的 **推理引擎对于模型部署的效率、性能及成本控制** 至关重要。
- **推理引擎** 负责优化模型的运行时表现，包括处理速度和资源利用率。本文将介绍并对比四种主流的推理引擎 **Ollama、vLLM、SGLang以及Hugging Face Pipeline** ，分析其技术特性、模型文件格式兼容性，并提供场景化的选型建议，以协助用户根据实际需求作出决策。

-------
Ollama
- 
- **模型小型化，显存占用低** & **简化的模型管理**：通过 Modelfile 统一管理， 方便新手

vLLM
- 
- by 创新的内存管理机制 **高并发与低延迟** & **广泛的兼容性** (支持与 Hugging Face 生态中的常见模型无缝集成，并兼容 NVIDIA、AMD 等主流 GPU 硬件。同时提供 OpenAI API 兼容的服务接口，便于应用迁移)& **进阶优化支持**：张量并行、流水线并行、推测解码及多种量化方案在内的优化技术

SGLang
- 
- 面向大型语言模型及*视觉语言*模型的高速推理引擎, 提供Pythonic接口，轻量化设计-还是得多节点

Hugging Face Pipeline
- 
- 是 transformers 库提供的一个高级 API，旨在 **简化各类预训练模型**（覆盖文本、图像、音频等模态）的推理应用。它封装了 **数据预处理**、**模型调用及后处理**等标准流程。

-------
模型文件格式与引擎匹配
- 
- *模型到底下载到本地是什么？?? *
  - 若模型以**GGUF**提供，优先选择 Ollama。
  - 若追求高性能并使用 **Safetensors 或 PyTorch** 格式的模型，推荐 vLLM 或 SGLang。
  - 若需要便捷调用多样化的模型或使用 **PyTorch 原生格式，Hugging Face Pipeline** 是首选。
  -  低门槛：Ollama 和 Hugging Face Pipeline 易于部署和使用，适合快速上手。
  -  高性能但需技术积累：vLLM 和 SGLang 在追求更高性能的同时，其配置和调优可能需要更深厚的技术积累。

-------
Torch 和 pyTorch 和 CUDA之间的关系
-
- **CUDA：显卡的超级计算器 for GPU**
  - CUDA（Compute Unified Device Architecture）是由NVIDIA推出的一种通用并行计算架构，它允许开发者 **使用GPU** 进行高效的并行计算。简而言之，CUDA将GPU变成了一个超级计算器，能够处理复杂的计算任务。 *然而，要使用CUDA，您的计算机必须配备NVIDIA的GPU，并且需要安装相应的驱动程序*。
  - GPU只可以理解硬件指令， CUDA 相当于需要将 **计算需求翻译成GPU可以理解的指令** ， 让GPU的算力利用起来
  - *可以理解成是个编译器？？？要使用CUDA，您的计算机必须配备NVIDIA的GPU*
  
- **Torch：Python的科学计算库**
  - Torch是一个 **基于Lua** 的科学计算库，提供了*大量的数学运算函数以及神经网络模型*。它最初由Facebook AI研究院（FAIR）开发，并逐渐被广泛应用于深度学习领域。然而，随着Python在数据科学和机器学习领域的普及，**Torch的Python版本——PyTorch也应运而生**。

- **PyTorch：Torch的Python版本**
  - PyTorch是Torch的Python实现，它继承了Torch的强大功能，并提供了更加 **Pythonic** 的接口。这使得PyTorch成为了一个非常受欢迎的深度学习框架。 **PyTorch具有动态计算图、高效的GPU支持以及易于使用的API** 等特点，使得开发者能够更加方便地进行模型开发和训练。
  - **深度学习的建模工具**，快速帮你搭建、训练、运行ai模型的工具包

- CUDA、Torch与PyTorch的关系
  - CUDA、Torch和PyTorch三者之间的关系可以概括为：*CUDA提供了GPU计算的能力，Torch（包括其Python版本PyTorch）则提供了深度学习模型的开发和训练工具。PyTorch可以充分利用CUDA的并行计算能力，使得深度学习模型的训练和推理更加高效。*
                                   
-----
小科普
- 
  - **所有电脑都有 GPU（图形处理器）**，用于处理图形显示和计算任务。
  - Mac电脑不使用NVIDIA显卡，目前主要有以下几种情况： 
    - M1、M2、M3、M4系列芯片都内置了Apple设计的GPU
    - Intel Mac（2020年前的老款）--intel,我的就是...  --此时就只可用cpu去跑模型了，模型层都是可以跑的，只是说模型的效率问题
    - 
  - Windows的GPU情况:
    -  NVIDIA --一般是游戏本
    -  AMD 性价比高，--一般是轻薄本
    -  还有Intel
    - 
  - **PyTorch 并不和 NVIDIA 强绑定**， Mac使用 **MPS（Metal Performance Shaders）** 代替CUDA
    PyTorch会自动检测并使用Apple GPU
  - **PyTorch支持多种GPU后端，不只是CUDA 噢😯！！！**  ，而且CPU也可以跑的，只是会比较慢   
  - ![img.png](img.png)
    
- 本地部署模型的技术栈建议 
  - 最简单方案：安装Ollama，自动处理一切
  - 进阶方案：CUDA + PyTorch + Hugging Face
  - Ollama和Hugging Face是获取/运行模型的工具，CUDA和PyTorch是底层支持
  - 

----- 
KV cache,兜兜转转,一切的技术底层离不开存储 gg 说错了🫢!!
-
- 稍等 kvcache 是数据库吗？ **⚠️ KV Cache 不是数据库!!!!**, **KV Cache（Key-Value Cache） 在大模型领域是一种内存优化技术，不是数据库。**
  - 向量数据库、关系型数据库、非关系型数据库、文档、图、事务型、弱事务
  - 持久化 和 非持久化
  - 倒排索引
  - 

----- 

































